# This file applies Q-learning to the data provided in data/
# under the format s,a,r,s' where s is the state, a is the action,
# r is the reward, and s' is the next state. The data is provided
# in the form of a csv file. The Q-learning algorithm is applied
# to the data and the resulting Q-table is saved in the form of
# a csv file.
# It also generates a plot of the Q-table for each state.
# And computes the optimal policy for each state, saves it in a
# csv file, and generates a plot of the policy for each state.

import numpy as np
from tqdm import tqdm
from utils import sample_outcome, sample_action, get_data_dict, get_data, save_policy
from scipy.spatial import cKDTree

# Suppress/hide the warning
np.seterr(invalid='ignore')

class NearestNeighborSmoothing:
    
    def __init__(self, k):
        self.k = k

    def fit(self, X, Y):
        self.X = X
        self.Y = Y
        self.kd_tree = cKDTree(X)

    def predict(self, x):
        distances, indices = self.kd_tree.query(x, k=self.k, workers=-1)
        # Uses a gaussian kernel to weight the neighbors
        weights = self.gaussian_kernel(distances)
        y_pred = np.sum(weights * self.Y[indices]) / np.sum(weights)
        return y_pred

    def gaussian_kernel(self, distances):
        return np.exp(-0.5 * ((distances / self.k) ** 2))


size = "large"
data = get_data(size)
data_dict = get_data_dict(size)
num_states = 312020
num_actions = 9

# Sorts the set of states and actiosn and create 4 dictionaries
# that map each state to an index and vice versa
# and each action to an index and vice versa
set_of_states = set(data['s'])
set_of_actions = set(data['a'])
sorted_set_of_states = sorted(set_of_states)
sorted_set_of_actions = sorted(set_of_actions)
action_to_index = {}
index_to_action = {}
for i in range(len(sorted_set_of_actions)):
    action = sorted_set_of_actions[i]
    action_to_index[action] = i
    index_to_action[i] = action

    

# Initialize Q-table
Q = np.zeros((num_states, num_actions))
mask = -np.inf * np.ones((num_states, num_actions))
for i in tqdm(range(data.shape[0]), desc="Generating mask"):
    # Get state, action, next state, and reward
    s = data.iloc[i, 0]
    a = data.iloc[i, 1]
    idx_s = s - 1
    idx_a = action_to_index[a]
    mask[idx_s, idx_a] = 0
# Prints dimensions of Q-table
print("Shape of Q-table: " + str(Q.shape))

# Kernel smoothing
normalizer = np.array([num_states, num_actions])
X = []
action_to_pos = [0.2, 0.25, 0.3, 0.35, 0.796, 0.797, 0.798, 0.799, 0.8]
normalizer = np.array([0.1, 0.1, 0.05, 0.01, 0.02, 0.01, 1])
for i in tqdm(range(data.shape[0]), desc="Kernel initialization"):
    # Get state, action, next state, and reward
    s = data.iloc[i, 0]
    a = data.iloc[i, 1]
    x = np.array([float(d) for d in str(s)] + [action_to_pos[a-1]]) * normalizer
    X.append(x)
X = np.array(X)

# Runs Q-learning if Q-table cannot be loaded
# Otherwise, loads Q-table

# Check if Q-table exists with os
import os
Q_path = "Q-table-" + size + ".npy"
if os.path.exists(Q_path):
    print("Loading Q-table...")
    Q = np.load(Q_path)
    print("Q-table loaded.")

else:
    print("Q-table not found. Running Q-learning...")
    repet = 0
    # Initialize parameters
    alpha = 0.05
    gamma = 0.95
    epsilon = 0.2
    num_episodes = 50000
    num_iter = 1000

    # Q-learning algorithm
    # (It is adapted since we do not contain all couples of state-action)
    for episode in tqdm(range(num_episodes), desc="Q-learning"):
        # Initialize to random state
        s = np.random.choice(sorted_set_of_states)
        idx_s = s - 1
        # Loop until the end of the episode
        for _ in range(num_iter):
            idx_a = None
            # Choose action
            if np.random.rand() < epsilon:
                a = sample_action(data_dict, s)
                idx_a = action_to_index[a]
            else:
                idx_a = np.argmax(Q[idx_s, :] + mask[idx_s, :])
                a = index_to_action[idx_a]
            # Get next state and reward
            (s_prime, r) = sample_outcome(data_dict, s, a)
            # Update Q-table
            idx_s_prime = s_prime - 1
            if s_prime not in set_of_states:
                if repet == 0:
                    Q[idx_s, idx_a] += alpha * (r - Q[idx_s, idx_a])
                else:
                    Q[idx_s, idx_a] += alpha * (r + gamma * np.max(Q[idx_s_prime, :]) - Q[idx_s, idx_a])
                break
            if repet == 0:
                Q[idx_s, idx_a] += alpha * (r + gamma * np.max(Q[idx_s_prime, :] + mask[idx_s_prime, :]) - Q[idx_s, idx_a])
            else:
                Q[idx_s, idx_a] += alpha * (r + gamma * np.max(Q[idx_s_prime, :]) - Q[idx_s, idx_a])
            # Update state
            s = s_prime
            idx_s = idx_s_prime

    # Saves Q-table
    np.save(Q_path, Q)

# Generates policy
policy = np.zeros(num_states, dtype=int)

# Kernel smoothing
Y = []
for i in tqdm(range(data.shape[0]), desc="Kernel initialization"):
    # Get state, action, next state, and reward
    s = data.iloc[i, 0]
    a = data.iloc[i, 1]
    idx_s = s - 1
    idx_a = action_to_index[a]
    Y.append(Q[idx_s, idx_a])
Y = np.array(Y)

ks = NearestNeighborSmoothing(k=100)
ks.fit(X, Y)


for idx_s in tqdm(range(num_states), desc="NN smoothing"):
    for idx_a in range(num_actions):
        s = 1 + idx_s
        str_s = str(s)
        str_s = "0" * (6 - len(str_s)) + str_s
        x = np.array([float(d) for d in str_s] + [action_to_pos[idx_a]]) * normalizer
        if mask[idx_s, idx_a] < 0:
            Q[idx_s, idx_a] = ks.predict(x)
        else:
            pass#Q[idx_s, idx_a] = (Q[idx_s, idx_a] + 2.0 * ks.predict(x)) / 3.0

policy = np.array([index_to_action[np.argmax(Q[idx_s, :])] for idx_s in range(num_states)], dtype=int)

# Compute optimal policy for each state
save_policy(policy, size)
